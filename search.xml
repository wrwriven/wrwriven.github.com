<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Pytorch</title>
      <link href="/posts/2807015082/"/>
      <url>/posts/2807015082/</url>
      
        <content type="html"><![CDATA[<h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><pre class="line-numbers language-none"><code class="language-none">import torchimport mathdtype &#x3D; torch.floatdevice &#x3D; torch.device(&quot;cpu&quot;)# device &#x3D; torch.device(&quot;cuda:0&quot;)x &#x3D; torch.linspace(-math.pi, math.pi, 2000, device&#x3D;device, dtype&#x3D;dtype)y &#x3D; torch.sin(x)a &#x3D; torch.randn( (), device&#x3D;device, dtype&#x3D;dtype, requiere_grad&#x3D;True)b &#x3D; torch.randn( (), device&#x3D;device, dtype&#x3D;dtype, requiere_grad&#x3D;True)c &#x3D; torch.randn( (), device&#x3D;device, dtype&#x3D;dtype, requiere_grad&#x3D;True)d &#x3D; torch.randn( (), device&#x3D;device, dtype&#x3D;dtype, requiere_grad&#x3D;True)learning_rate &#x3D; 1e-6for t in range(2000)y_pred &#x3D; a + b * x + c * x ** 2 + d * x ** 3loss &#x3D; (y_pred - y).pow(2).sum()if t % 100 &#x3D;&#x3D; 99:print(t, loss.item())loss.backward()with torch.no_grad():a -&#x3D; learning_rate * a.gradb -&#x3D; learning_rate * b.gradc -&#x3D; learning_rate * c.gradd -&#x3D; learning_rate * d.grada.grad &#x3D; Noneb.grad &#x3D; Nonec.grad &#x3D; Noned.grad &#x3D; Noneprint(f&quot;Result: y &#x3D; &#123;a.item()&#125; + &#123;b.item()&#125;x + &#123;c.item()&#125;x^2 + &#123;d.item()&#125;x^3&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="定义Autograd函数"><a href="#定义Autograd函数" class="headerlink" title="定义Autograd函数"></a>定义Autograd函数</h2><pre class="line-numbers language-none"><code class="language-none">import torchimport mathclass LegendrePolynomial3(torch.autograd.Function):    &quot;&quot;&quot;    We can implement our own custom autograd Functions by subclassing    torch.autograd.Function and implementing the forward and backward passes    which operate on Tensors.    &quot;&quot;&quot;    @staticmethod    def forward(ctx, input):        &quot;&quot;&quot;        In the forward pass we receive a Tensor containing the input and return        a Tensor containing the output. ctx is a context object that can be used        to stash information for backward computation. You can cache arbitrary        objects for use in the backward pass using the ctx.save_for_backward method.        &quot;&quot;&quot;        ctx.save_for_backward(input)        return 0.5 * (5 * input ** 3 - 3 * input)    @staticmethod    def backward(ctx, grad_output):        &quot;&quot;&quot;        In the backward pass we receive a Tensor containing the gradient of the loss        with respect to the output, and we need to compute the gradient of the loss        with respect to the input.        &quot;&quot;&quot;        input, &#x3D; ctx.saved_tensors        return grad_output * 1.5 * (5 * input ** 2 - 1)dtype &#x3D; torch.floatdevice &#x3D; torch.device(&quot;cpu&quot;)# device &#x3D; torch.device(&quot;cuda:0&quot;)  # Uncomment this to run on GPU# Create Tensors to hold input and outputs.# By default, requires_grad&#x3D;False, which indicates that we do not need to# compute gradients with respect to these Tensors during the backward pass.x &#x3D; torch.linspace(-math.pi, math.pi, 2000, device&#x3D;device, dtype&#x3D;dtype)y &#x3D; torch.sin(x)# Create random Tensors for weights. For this example, we need# 4 weights: y &#x3D; a + b * P3(c + d * x), these weights need to be initialized# not too far from the correct result to ensure convergence.# Setting requires_grad&#x3D;True indicates that we want to compute gradients with# respect to these Tensors during the backward pass.a &#x3D; torch.full((), 0.0, device&#x3D;device, dtype&#x3D;dtype, requires_grad&#x3D;True)b &#x3D; torch.full((), -1.0, device&#x3D;device, dtype&#x3D;dtype, requires_grad&#x3D;True)c &#x3D; torch.full((), 0.0, device&#x3D;device, dtype&#x3D;dtype, requires_grad&#x3D;True)d &#x3D; torch.full((), 0.3, device&#x3D;device, dtype&#x3D;dtype, requires_grad&#x3D;True)learning_rate &#x3D; 5e-6for t in range(2000):    # To apply our Function, we use Function.apply method. We alias this as &#39;P3&#39;.    P3 &#x3D; LegendrePolynomial3.apply    # Forward pass: compute predicted y using operations; we compute    # P3 using our custom autograd operation.    y_pred &#x3D; a + b * P3(c + d * x)    # Compute and print loss    loss &#x3D; (y_pred - y).pow(2).sum()    if t % 100 &#x3D;&#x3D; 99:        print(t, loss.item())    # Use autograd to compute the backward pass.    loss.backward()    # Update weights using gradient descent    with torch.no_grad():        a -&#x3D; learning_rate * a.grad        b -&#x3D; learning_rate * b.grad        c -&#x3D; learning_rate * c.grad        d -&#x3D; learning_rate * d.grad        # Manually zero the gradients after updating weights        a.grad &#x3D; None        b.grad &#x3D; None        c.grad &#x3D; None        d.grad &#x3D; Noneprint(f&#39;Result: y &#x3D; &#123;a.item()&#125; + &#123;b.item()&#125; * P3(&#123;c.item()&#125; + &#123;d.item()&#125; x)&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="PYTHORCH-NN"><a href="#PYTHORCH-NN" class="headerlink" title="PYTHORCH: NN"></a>PYTHORCH: NN</h2><p>这个实现使用 PyTorch 的nn包来构建神经网络。 PyTorch Autograd 让我们定义计算图和计算梯度变得容易了，但是原始的 Autograd 对于定义复杂的神经网络来说可能太底层了。 这时候nn包就能帮上忙。 nn包定义了一组模块，你可以把它视作一层神经网络，该神经网络层接受输入，产生输出，并且可能有一些可训练的权重。</p><pre class="line-numbers language-none"><code class="language-none">import torchimport math# 创建张量，输入和输出x &#x3D; torch.linspace(-math.pi, math.pi, 2000)y &#x3D; torch.sin(x)# 对于这个例子用了线性回归# 把它作为一个神经网络# 定义了张量tensor (x, x^2, x^3).p &#x3D; torch.tensor([1, 2, 3])xx &#x3D; x.unsqueeze(-1).pow(p)# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape# (3,), for this case, broadcasting semantics will apply to obtain a tensor# of shape (2000, 3)# Use the nn package to define our model as a sequence of layers. nn.Sequential# is a Module which contains other Modules, and applies them in sequence to# produce its output. The Linear Module computes output from input using a# linear function, and holds internal Tensors for its weight and bias.# The Flatten layer flatens the output of the linear layer to a 1D tensor,# to match the shape of &#96;y&#96;.model &#x3D; torch.nn.Sequential(    torch.nn.Linear(3, 1),    torch.nn.Flatten(0, 1))# The nn package also contains definitions of popular loss functions; in this# case we will use Mean Squared Error (MSE) as our loss function.loss_fn &#x3D; torch.nn.MSELoss(reduction&#x3D;&#39;sum&#39;)learning_rate &#x3D; 1e-6for t in range(2000):    # Forward pass: compute predicted y by passing x to the model. Module objects    # override the __call__ operator so you can call them like functions. When    # doing so you pass a Tensor of input data to the Module and it produces    # a Tensor of output data.    y_pred &#x3D; model(xx)    # Compute and print loss. We pass Tensors containing the predicted and true    # values of y, and the loss function returns a Tensor containing the    # loss.    loss &#x3D; loss_fn(y_pred, y)    if t % 100 &#x3D;&#x3D; 99:        print(t, loss.item())    # Zero the gradients before running the backward pass.    model.zero_grad()    # Backward pass: compute gradient of the loss with respect to all the learnable    # parameters of the model. Internally, the parameters of each Module are stored    # in Tensors with requires_grad&#x3D;True, so this call will compute gradients for    # all learnable parameters in the model.    loss.backward()    # Update the weights using gradient descent. Each parameter is a Tensor, so    # we can access its gradients like we did before.    with torch.no_grad():        for param in model.parameters():            param -&#x3D; learning_rate * param.grad# You can access the first layer of &#96;model&#96; like accessing the first item of a listlinear_layer &#x3D; model[0]# For linear layer, its parameters are stored as &#96;weight&#96; and &#96;bias&#96;.print(f&#39;Result: y &#x3D; &#123;linear_layer.bias.item()&#125; + &#123;linear_layer.weight[:, 0].item()&#125; x + &#123;linear_layer.weight[:, 1].item()&#125; x^2 + &#123;linear_layer.weight[:, 2].item()&#125; x^3&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/1243066710/"/>
      <url>/posts/1243066710/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
